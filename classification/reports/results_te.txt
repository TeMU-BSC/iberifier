recognai tweets
{'entailment', 'neutral', 'contradiction'} 587
{'entailment', 'neutral', 'contradiction'} 587
               precision    recall  f1-score   support

contradiction       0.42      0.52      0.46       199
   entailment       0.50      0.34      0.41       166
      neutral       0.44      0.45      0.45       222

     accuracy                           0.44       587
    macro avg       0.45      0.44      0.44       587
 weighted avg       0.45      0.44      0.44       587

0.44293015332197616
[[ 57  49  60]
 [ 29 103  67]
 [ 27  95 100]]
roberta tweets
{'contradiction', 'entailment', 'not_entailment'} 587
{'contradiction', 'entailment', 'not_entailment'} 587
                precision    recall  f1-score   support

 contradiction       0.57      0.42      0.49       199
    entailment       0.52      0.28      0.37       166
not_entailment       0.43      0.68      0.53       222

      accuracy                           0.48       587
     macro avg       0.51      0.46      0.46       587
  weighted avg       0.50      0.48      0.47       587

0.4804088586030664
[[ 47  25  94]
 [ 10  84 105]
 [ 33  38 151]]
xlm tweets
{'contradiction', 'neutral', 'entailment'} 587
{'contradiction', 'neutral', 'entailment'} 587
               precision    recall  f1-score   support

contradiction       0.61      0.52      0.56       199
   entailment       0.61      0.46      0.53       166
      neutral       0.51      0.66      0.58       222

     accuracy                           0.56       587
    macro avg       0.57      0.55      0.55       587
 weighted avg       0.57      0.56      0.56       587

0.5587734241908007
[[ 77  24  65]
 [ 18 104  77]
 [ 32  43 147]]
